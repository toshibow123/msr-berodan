#!/usr/bin/env python3
"""
MGStageã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å®˜èƒ½å°èª¬ã‚µã‚¤ãƒˆã€Œè‰¶ã‚ãç‰©èªã€å°‚ç”¨
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import random
import os
import sys
from urllib.parse import urljoin
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
try:
    script_dir = Path(__file__).parent
    project_root = script_dir.parent
    env_path = project_root / ".env"
    if env_path.exists():
        load_dotenv(env_path)
        print(f"âœ… .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {env_path}")
    else:
        print(f"âš ï¸  .envãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {env_path}")
except ImportError:
    print("âš ï¸  python-dotenvãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚pip install python-dotenv ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
except Exception as e:
    print(f"âš ï¸  .envãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

# ==========================================
# è¨­å®š
# ==========================================
# ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆIDï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
AFFILIATE_ID = os.environ.get("MGS_AFFILIATE_ID")

if not AFFILIATE_ID:
    print("âŒ ç’°å¢ƒå¤‰æ•° MGS_AFFILIATE_ID ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“", file=sys.stderr)
    print("   .envãƒ•ã‚¡ã‚¤ãƒ«ã« MGS_AFFILIATE_ID=YOUR_AFFILIATE_ID ã‚’è¨­å®šã—ã¦ãã ã•ã„", file=sys.stderr)
    sys.exit(1)

# ä¿å­˜å…ˆ
OUTPUT_DIR = Path(__file__).parent.parent / "data"
OUTPUT_FILE = OUTPUT_DIR / "mgs_scraped_data.json"
# å–å¾—ãƒšãƒ¼ã‚¸æ•°
MAX_PAGES = 5  # ãƒ†ã‚¹ãƒˆç”¨ã«å°‘ãªã‚ã€‚æœ¬ç•ªã¯100ã¨ã‹ã«ã™ã‚‹

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆDMMã¨åŒã˜ã‚ˆã†ãªãƒ¯ãƒ¼ãƒ‰ï¼‰
DEFAULT_SEARCH_KEYWORDS = [
    "äººå¦»",
    "ç†Ÿå¥³",
    "NTR",
    "ãƒãƒˆãƒ©ãƒ¬",
    "ãƒ‰ãƒ©ãƒ",
    "ä¸»å©¦"
]

def scrape_mgs(max_pages=MAX_PAGES, search_word=None, search_keywords=None, affiliate_id=None):
    """
    MGStageã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
    
    Args:
        max_pages: å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚ãŸã‚Šã®å–å¾—ãƒšãƒ¼ã‚¸æ•°
        search_word: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆå˜ä¸€ã€å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ®‹ã™ï¼‰
        search_keywords: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆï¼ˆè¤‡æ•°å¯¾å¿œï¼‰
        affiliate_id: ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆIDï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€æŒ‡å®šã•ã‚Œãªã„å ´åˆã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‚’ä½¿ç”¨ï¼‰
    """
    # ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆIDã®æ±ºå®š
    actual_affiliate_id = affiliate_id or AFFILIATE_ID
    if not actual_affiliate_id:
        print("âŒ ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆIDãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“", file=sys.stderr)
        return []
    
    # æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æ±ºå®š
    if search_keywords:
        keywords = search_keywords
    elif search_word:
        keywords = [search_word]
    else:
        keywords = DEFAULT_SEARCH_KEYWORDS
    
    base_url = "https://www.mgstage.com/search/search.php"
    results = []
    seen_content_ids = set()  # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆï¼ˆå¹´é½¢èªè¨¼ã®ã‚¯ãƒƒã‚­ãƒ¼ã‚’ä¿æŒã™ã‚‹ãŸã‚ï¼‰
    session = requests.Session()
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    session.headers.update(headers)
    
    # å¹´é½¢èªè¨¼ã‚’é€šéã™ã‚‹ï¼ˆã‚¯ãƒƒã‚­ãƒ¼ã‚’è¨­å®šï¼‰
    print("ğŸ” å¹´é½¢èªè¨¼ã‚’é€šéä¸­...")
    try:
        # ã¾ãšå¹´é½¢èªè¨¼ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
        age_check_url = "https://www.mgstage.com/search/search.php"
        age_check_response = session.get(age_check_url, timeout=10)
        
        # å¹´é½¢èªè¨¼ã®ã‚¯ãƒƒã‚­ãƒ¼ã‚’è¨­å®šï¼ˆadc=1ï¼‰
        session.cookies.set('adc', '1', domain='mgstage.com', path='/')
        
        # å†åº¦ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦èªè¨¼ãŒé€šã£ãŸã‹ç¢ºèª
        test_response = session.get(age_check_url, timeout=10)
        test_soup = BeautifulSoup(test_response.content, "html.parser")
        
        # å¹´é½¢èªè¨¼ãƒšãƒ¼ã‚¸ã‹ã©ã†ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆ"å¹´é½¢èªè¨¼"ã¨ã„ã†ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹ã‹ï¼‰
        if "å¹´é½¢èªè¨¼" in test_response.text:
            print("âš ï¸  å¹´é½¢èªè¨¼ãŒã¾ã å¿…è¦ã§ã™ã€‚æ‰‹å‹•ã§ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        else:
            print("âœ… å¹´é½¢èªè¨¼ã‚’é€šéã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"âš ï¸  å¹´é½¢èªè¨¼ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        print("   ç¶šè¡Œã—ã¾ã™ãŒã€å¹´é½¢èªè¨¼ãŒå¿…è¦ãªå ´åˆãŒã‚ã‚Šã¾ã™...")

    print("ğŸš€ MGSãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™...")
    print(f"ğŸ“Š å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚ãŸã‚Šã®å–å¾—ãƒšãƒ¼ã‚¸æ•°: {max_pages}ãƒšãƒ¼ã‚¸")
    print(f"ğŸ”— ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆID: {actual_affiliate_id}")
    print(f"ğŸ” æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(keywords)}")
    print(f"ğŸ“ åˆè¨ˆ {len(keywords)}å€‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢ã—ã¾ã™\n")

    # å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢ã‚’å®Ÿè¡Œ
    for keyword_idx, keyword in enumerate(keywords, 1):
        print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        print(f"ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ {keyword_idx}/{len(keywords)}: ã€Œ{keyword}ã€")
        print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        
        keyword_results = 0
        
        for page in range(1, max_pages + 1):
            # æ¤œç´¢æ¡ä»¶ï¼ˆä¾‹ï¼šé…ä¿¡é–‹å§‹æ—¥é †ã€å…¨ã‚¸ãƒ£ãƒ³ãƒ«ï¼‰
            url = f"{base_url}?sort=new&page={page}&search_word={keyword}"
            
            try:
                print(f"ğŸ“„ ãƒšãƒ¼ã‚¸ {page}/{max_pages} ã‚’å–å¾—ä¸­: {url}")
                
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆã‚¯ãƒƒã‚­ãƒ¼ãŒä¿æŒã•ã‚Œã‚‹ï¼‰
                response = session.get(url, timeout=10)
                response.raise_for_status()
                response.encoding = response.apparent_encoding or 'utf-8'
                
                soup = BeautifulSoup(response.content, "html.parser")
                
                # MGStageã®å•†å“ãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆæ­£ã—ã„ã‚»ãƒ¬ã‚¯ã‚¿ï¼‰
                products = soup.select(".product_list_item")
                
                if not products:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å•†å“ãƒªãƒ³ã‚¯ã‹ã‚‰æ¢ã™
                    all_links = soup.select("a[href*='/product/product_detail/']")
                    if all_links:
                        print(f"   ğŸ” å•†å“ãƒªãƒ³ã‚¯ã‚’ {len(all_links)}ä»¶è¦‹ã¤ã‘ã¾ã—ãŸï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰")
                        # ãƒªãƒ³ã‚¯ã®è¦ªè¦ç´ ã‚’å•†å“ã¨ã—ã¦æ‰±ã†ï¼ˆ.product_list_item ã‚’æ¢ã™ï¼‰
                        products = []
                        for link in all_links:
                            # è¦ªè¦ç´ ã‚’é¡ã£ã¦ .product_list_item ã‚’æ¢ã™
                            parent = link.parent
                            while parent:
                                if 'product_list_item' in ' '.join(parent.get('class', [])):
                                    if parent not in products:
                                        products.append(parent)
                                    break
                                parent = parent.parent if parent.parent else None
                        # ãã‚Œã§ã‚‚è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ãƒªãƒ³ã‚¯ã®è¦ªè¦ç´ ã‚’ä½¿ç”¨
                        if not products:
                            products = [link.parent for link in all_links if link.parent]
                
                if products:
                    print(f"   ğŸ” å•†å“ã‚’ {len(products)}ä»¶è¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
                
                # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šå¸¸ã«HTMLã‚’ä¿å­˜ï¼ˆæœ€åˆã®ãƒšãƒ¼ã‚¸ã®ã¿ï¼‰
                if page == 1 and keyword_idx == 1:
                    debug_file = OUTPUT_DIR / f"mgs_debug_{keyword.replace('/', '_')}_{page}.html"
                    OUTPUT_DIR.mkdir(exist_ok=True)
                    try:
                        with open(debug_file, "w", encoding="utf-8") as f:
                            f.write(soup.prettify())
                        print(f"   ğŸ’¾ ãƒ‡ãƒãƒƒã‚°ç”¨HTMLã‚’ä¿å­˜ã—ã¾ã—ãŸ: {debug_file}")
                    except Exception as e:
                        print(f"   âš ï¸  HTMLä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
                
                if not products:
                    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å‡ºåŠ›
                    print(f"   âš ï¸  å•†å“ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚HTMLæ§‹é€ ã‚’ç¢ºèªã—ã¾ã™...")
                    # ä¸»è¦ãªã‚¯ãƒ©ã‚¹åã‚’æ¢ã™
                    all_classes = set()
                    for tag in soup.find_all(class_=True):
                        all_classes.update(tag.get('class', []))
                    print(f"   ğŸ“‹ è¦‹ã¤ã‹ã£ãŸã‚¯ãƒ©ã‚¹åï¼ˆä¸€éƒ¨ï¼‰: {sorted(list(all_classes))[:15]}")
                    # ãƒªã‚¹ãƒˆè¦ç´ ã‚’æ¢ã™
                    lists = soup.find_all(['ul', 'ol', 'div'], class_=True)
                    print(f"   ğŸ“‹ ãƒªã‚¹ãƒˆè¦ç´ : {len(lists)}ä»¶")
                    if lists:
                        for lst in lists[:5]:
                            classes = ' '.join(lst.get('class', []))
                            if classes:
                                print(f"      <{lst.name} class='{classes}'>")
                    # HTMLã‚’ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                    debug_file = OUTPUT_DIR / f"mgs_debug_{keyword}_{page}.html"
                    OUTPUT_DIR.mkdir(exist_ok=True)
                    with open(debug_file, "w", encoding="utf-8") as f:
                        f.write(soup.prettify())
                    print(f"   ğŸ’¾ ãƒ‡ãƒãƒƒã‚°ç”¨HTMLã‚’ä¿å­˜ã—ã¾ã—ãŸ: {debug_file}")
                    print("âš ï¸ å•†å“ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ¬¡ã®ãƒšãƒ¼ã‚¸ã¸...")
                    continue

                page_count = 0
                for product in products:
                    try:
                        # å•†å“ãƒªãƒ³ã‚¯ã‚’æ¢ã™ï¼ˆ/product/product_detail/ã‚’å«ã‚€ã‚‚ã®ã®ã¿ï¼‰
                        product_link = None
                        if product.name == "a":
                            # å•†å“è‡ªä½“ãŒaã‚¿ã‚°ã®å ´åˆ
                            href = product.get("href", "")
                            if "/product/product_detail/" in href:
                                product_link = product
                        else:
                            # å•†å“å†…ã®aã‚¿ã‚°ã‚’æ¢ã™ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ãƒªãƒ³ã‚¯ã‚’å„ªå…ˆï¼‰
                            title_link = product.select_one("a.title")
                            if title_link and "/product/product_detail/" in title_link.get("href", ""):
                                product_link = title_link
                            else:
                                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å•†å“å†…ã®/product/product_detail/ã‚’å«ã‚€ãƒªãƒ³ã‚¯ã‚’æ¢ã™
                                links = product.select("a[href*='/product/product_detail/']")
                                if links:
                                    product_link = links[0]
                        
                        if not product_link:
                            continue
                        
                        link = product_link.get("href", "")
                        if not link or "/product/product_detail/" not in link:
                            continue
                        
                        # ç›¸å¯¾URLã®å ´åˆã¯çµ¶å¯¾URLã«å¤‰æ›
                        if not link.startswith("http"):
                            link = urljoin("https://www.mgstage.com", link)
                        
                        # ã‚¿ã‚¤ãƒˆãƒ«ã®æŠ½å‡ºï¼ˆ.title ã‚¯ãƒ©ã‚¹ã®aã‚¿ã‚°ã‹ã‚‰ï¼‰
                        title = ""
                        title_tag = product.select_one("a.title")
                        if title_tag:
                            # spanè¦ç´ ã‚’é™¤å¤–ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                            title = title_tag.get_text(strip=True)
                            # ãƒœã‚¿ãƒ³ãªã©ã®spanã‚’é™¤å¤–
                            for span in title_tag.select("span"):
                                span_text = span.get_text(strip=True)
                                if span_text:
                                    title = title.replace(span_text, "").strip()
                        
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ä»–ã®æ–¹æ³•ã§ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ¢ã™
                        if not title:
                            title = product_link.text.strip()
                        if not title:
                            title_tag = product.select_one("h3, h4, h5, h6")
                            if title_tag:
                                title = title_tag.text.strip()
                        if not title:
                            img_tag = product.select_one("img")
                            if img_tag:
                                title = img_tag.get("alt", "").strip()
                        
                        if not title:
                            # æœ€å¾Œã®æ‰‹æ®µï¼šURLã‹ã‚‰æ¨æ¸¬
                            parts = link.split("/")
                            if parts:
                                title = parts[-1].replace(".html", "").replace("_", " ")
                        
                        if not title:
                            continue
                        
                        # å•†å“è©³ç´°URLã«ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆIDã‚’ä»˜ä¸
                        separator = "&" if "?" in link else "?"
                        affiliate_url = f"{link}{separator}af={actual_affiliate_id}"
                        
                        # ç”»åƒURLã®æŠ½å‡º
                        image_url = ""
                        # ã¾ãšå•†å“è¦ç´ å†…ã®imgã‚’æ¢ã™
                        image_tag = product.select_one("img")
                        if not image_tag and product_link:
                            # aã‚¿ã‚°ã®è¦ªè¦ç´ ã‹ã‚‰æ¢ã™
                            parent = product_link.parent
                            if parent:
                                image_tag = parent.select_one("img")
                        
                        if image_tag:
                            image_url = image_tag.get("src") or image_tag.get("data-src") or image_tag.get("data-original") or ""
                            if image_url:
                                # ç›¸å¯¾URLã®å ´åˆã¯çµ¶å¯¾URLã«å¤‰æ›
                                if not image_url.startswith("http"):
                                    image_url = urljoin("https://www.mgstage.com", image_url)
                        
                        # content_idã‚’æŠ½å‡ºï¼ˆURLã‹ã‚‰ï¼‰
                        content_id = ""
                        if "/product/product_detail/" in link:
                            # /product/product_detail/XXXXXX/ ã®å½¢å¼ã‹ã‚‰æŠ½å‡º
                            parts = link.split("/product/product_detail/")
                            if len(parts) > 1:
                                content_id = parts[1].split("/")[0].split("?")[0]
                        elif "/product/detail/" in link:
                            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: /product/detail/XXXXXX/ ã®å½¢å¼ã‹ã‚‰æŠ½å‡º
                            parts = link.split("/product/detail/")
                            if len(parts) > 1:
                                content_id = parts[1].split("/")[0].split("?")[0]
                        elif "/product/" in link:
                            parts = link.split("/product/")
                            if len(parts) > 1:
                                content_id = parts[1].split("/")[0].split("?")[0]
                        elif "/" in link:
                            parts = link.rstrip("/").split("/")
                            content_id = parts[-1].split("?")[0].replace(".html", "")
                        
                        # ç™ºå£²æ—¥ã®å–å¾—ï¼ˆå¯èƒ½ã§ã‚ã‚Œã°ï¼‰
                        date_tag = product.select_one(".date, .release-date, time")
                        release_date = ""
                        if date_tag:
                            release_date = date_tag.text.strip()
                        else:
                            release_date = datetime.now().strftime("%Y-%m-%d")
                        
                        # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆcontent_idãŒæ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
                        if content_id and content_id in seen_content_ids:
                            continue
                        if content_id:
                            seen_content_ids.add(content_id)
                        
                        # DMMã®JSONå½¢å¼ã«åˆã‚ã›ã‚‹ï¼ˆã“ã“ãŒé‡è¦ï¼‰
                        item_data = {
                            "rank": len(results) + 1,
                            "content_id": content_id or f"mgs_{len(results) + 1}",
                            "title": title,
                            "url": link,
                            "affiliate_url": affiliate_url,
                            "image_url": image_url,
                            "price": "",
                            "release_date": release_date,
                            "actress": [],
                            "genre": [],
                            "maker": "",
                            "director": "",
                            "description": "",
                            "service": "MGS",  # åˆ¤åˆ¥ç”¨ã‚¿ã‚°
                            "search_keyword": keyword  # ã©ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§è¦‹ã¤ã‹ã£ãŸã‹
                        }
                        results.append(item_data)
                        page_count += 1
                        keyword_results += 1
                        
                    except Exception as e:
                        print(f"      âš ï¸  å•†å“ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                        continue

                print(f"   âœ… {page_count}ä»¶ã®å•†å“ã‚’å–å¾—ã—ã¾ã—ãŸï¼ˆã“ã®ãƒšãƒ¼ã‚¸ï¼‰")
                
                # ã‚µãƒ¼ãƒãƒ¼è² è·å¯¾ç­–ï¼ˆé‡è¦ï¼‰
                if page < max_pages:
                    sleep_time = random.uniform(1.0, 3.0)
                    print(f"   â³ {sleep_time:.1f}ç§’å¾…æ©Ÿä¸­...")
                    time.sleep(sleep_time)

            except requests.exceptions.RequestException as e:
                print(f"âŒ ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
                break
            except Exception as e:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
                import traceback
                traceback.print_exc()
                break
        
        print(f"âœ… ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{keyword}ã€ã§ {keyword_results}ä»¶ã®å•†å“ã‚’å–å¾—ã—ã¾ã—ãŸ")
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é–“ã®å¾…æ©Ÿæ™‚é–“
        if keyword_idx < len(keywords):
            sleep_time = random.uniform(2.0, 4.0)
            print(f"â³ æ¬¡ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¾ã§ {sleep_time:.1f}ç§’å¾…æ©Ÿä¸­...\n")
            time.sleep(sleep_time)

    # ä¿å­˜
    OUTPUT_DIR.mkdir(exist_ok=True)
    
    output_data = {
        "fetched_at": datetime.now().isoformat(),
        "total_count": len(results),
        "affiliate_id": actual_affiliate_id,
        "ranking": results
    }
    
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… å®Œäº†ï¼ {len(results)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {OUTPUT_FILE}")
    return results

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="MGStageã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°")
    parser.add_argument(
        "--pages",
        type=int,
        default=MAX_PAGES,
        help="å–å¾—ã™ã‚‹ãƒšãƒ¼ã‚¸æ•°"
    )
    parser.add_argument(
        "--search-word",
        type=str,
        help="æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆå˜ä¸€ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰"
    )
    parser.add_argument(
        "--search-keywords",
        type=str,
        nargs="+",
        help="æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆè¤‡æ•°æŒ‡å®šå¯èƒ½ã€ä¾‹: --search-keywords äººå¦» ç†Ÿå¥³ NTRï¼‰"
    )
    parser.add_argument(
        "--use-default-keywords",
        action="store_true",
        help="ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ï¼ˆäººå¦»ã€ç†Ÿå¥³ã€NTRã€ãƒãƒˆãƒ©ãƒ¬ã€ãƒ‰ãƒ©ãƒã€ä¸»å©¦ï¼‰"
    )
    parser.add_argument(
        "--affiliate-id",
        type=str,
        help="ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆIDï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€ç’°å¢ƒå¤‰æ•°MGS_AFFILIATE_IDãŒå„ªå…ˆï¼‰"
    )
    
    args = parser.parse_args()
    
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§æŒ‡å®šã•ã‚ŒãŸå ´åˆã¯ãã‚Œã‚’ä½¿ç”¨
    affiliate_id = args.affiliate_id or AFFILIATE_ID
    
    if not affiliate_id:
        print("âŒ ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆIDãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“", file=sys.stderr)
        print("   --affiliate-id ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§æŒ‡å®šã™ã‚‹ã‹ã€.envãƒ•ã‚¡ã‚¤ãƒ«ã« MGS_AFFILIATE_ID ã‚’è¨­å®šã—ã¦ãã ã•ã„", file=sys.stderr)
        sys.exit(1)
    
    # æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æ±ºå®š
    search_keywords = None
    if args.search_keywords:
        search_keywords = args.search_keywords
    elif args.use_default_keywords:
        search_keywords = DEFAULT_SEARCH_KEYWORDS
    elif args.search_word:
        search_keywords = [args.search_word]
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§è¤‡æ•°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä½¿ç”¨
        search_keywords = DEFAULT_SEARCH_KEYWORDS
    
    scrape_mgs(
        max_pages=args.pages, 
        search_word=args.search_word,  # å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ®‹ã™
        search_keywords=search_keywords,
        affiliate_id=affiliate_id
    )

