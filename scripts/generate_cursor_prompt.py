#!/usr/bin/env python3
"""
DMMä½œå“URLã‹ã‚‰æƒ…å ±ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã€Cursorï¼ˆAIï¼‰ã«è¨˜äº‹ã‚’æ›¸ã‹ã›ã‚‹ãŸã‚ã®
è©³ç´°ãªæŒ‡ç¤ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã—ã¦ã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰ã«ã‚³ãƒ”ãƒ¼ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import re
import sys
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from urllib.parse import urlparse, parse_qs, unquote
import pyperclip


def extract_content_id_from_url(url: str) -> str | None:
    """
    URLã‹ã‚‰å“ç•ªï¼ˆcontent_idï¼‰ã‚’æŠ½å‡ºï¼ˆã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆãƒªãƒ³ã‚¯å¯¾å¿œç‰ˆï¼‰
    
    Args:
        url: DMMä½œå“URL
        
    Returns:
        content_id ã¾ãŸã¯ None
    """
    # ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆãƒªãƒ³ã‚¯ã®å ´åˆã€å®ŸURLã‚’å–ã‚Šå‡ºã™
    if "al.fanza.co.jp" in url or "al.dmm.co.jp" in url:
        parsed = urlparse(url)
        qs = parse_qs(parsed.query)
        if 'lurl' in qs:
            url = unquote(qs['lurl'][0])
            print(f"ğŸ” ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆãƒªãƒ³ã‚¯ã‚’æ¤œå‡º: å®ŸURLã«å¤‰æ›ã—ã¾ã—ãŸ")
    
    # æ­£è¦è¡¨ç¾ã§å“ç•ªã‚’æŠ½å‡ºï¼ˆè¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œï¼‰
    patterns = [
        r'cid=([a-z0-9_]+)',        # é€šå¸¸ã®DMM: /cid=abc123/
        r'id=([a-z0-9_]+)',          # å‹•ç”»é…ä¿¡: /id=abc123/
        r'/detail/=/cid=([a-z0-9_]+)', # ãƒ‘ã‚¹åŸ‹ã‚è¾¼ã¿å½¢å¼
        r'content_id=([a-z0-9_]+)',  # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å½¢å¼
    ]
    
    for pattern in patterns:
        match = re.search(pattern, url, re.IGNORECASE)
        if match:
            return match.group(1)
    
    return None


def scrape_dmm_product_info(url: str) -> dict | None:
    """
    DMMä½œå“ãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
    
    Args:
        url: DMMä½œå“URL
        
    Returns:
        ä½œå“æƒ…å ±ã®è¾æ›¸ã€ã¾ãŸã¯ None
    """
    try:
        # age_check_done=1 cookieã‚’è¨­å®š
        cookies = {
            'age_check_done': '1'
        }
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
        print("ğŸ“¡ ãƒšãƒ¼ã‚¸ã‚’å–å¾—ä¸­...")
        response = requests.get(url, cookies=cookies, headers=headers, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ä½œå“åã‚’å–å¾—
        title = ""
        title_elem = soup.select_one('h1#title, h1.title, .itemBox h1, .itemBox .title')
        if title_elem:
            title = title_elem.get_text(strip=True)
        else:
            # ã‚¿ã‚¤ãƒˆãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€metaã‚¿ã‚°ã‹ã‚‰å–å¾—
            meta_title = soup.select_one('meta[property="og:title"]')
            if meta_title:
                title = meta_title.get('content', '').strip()
        
        if not title:
            print("âš ï¸  ä½œå“åãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        
        # ä½œå“ID (CID) ã‚’å–å¾—
        content_id = extract_content_id_from_url(url)
        if not content_id:
            # URLã‹ã‚‰å–å¾—ã§ããªã„å ´åˆã€ãƒšãƒ¼ã‚¸å†…ã‹ã‚‰æ¢ã™
            cid_pattern = re.search(r'cid[=:](\w+)', response.text, re.IGNORECASE)
            if cid_pattern:
                content_id = cid_pattern.group(1)
        
        # ç´¹ä»‹æ–‡ã‚’å–å¾—
        description = ""
        # è¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ç´¹ä»‹æ–‡ã‚’æ¢ã™
        desc_selectors = [
            '.itemBox .itemText',
            '.itemBox .description',
            '.itemBox .review',
            '#itemText',
            '.itemText',
            '.description',
            '.review'
        ]
        
        for selector in desc_selectors:
            desc_elem = soup.select_one(selector)
            if desc_elem:
                description = desc_elem.get_text(strip=True)
                if description:
                    break
        
        # ã¾ã è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€meta descriptionã‹ã‚‰å–å¾—
        if not description:
            meta_desc = soup.select_one('meta[name="description"], meta[property="og:description"]')
            if meta_desc:
                description = meta_desc.get('content', '').strip()
        
        # ä½œå“ç‰¹å¾´ï¼ˆã‚¿ã‚°æƒ…å ±ï¼‰ã‚’å–å¾—
        keywords = []
        
        # å‡ºæ¼”è€…ã‚’å–å¾—
        actresses = []
        actress_elems = soup.select('.actressName, .actress, .performer, [data-actress]')
        for elem in actress_elems:
            name = elem.get_text(strip=True)
            if name and name not in actresses:
                actresses.append(name)
        
        # ã‚¸ãƒ£ãƒ³ãƒ«ã‚’å–å¾—
        genres = []
        genre_elems = soup.select('.genre, .category, [data-genre]')
        for elem in genre_elems:
            genre = elem.get_text(strip=True)
            if genre and genre not in genres:
                genres.append(genre)
        
        # ãƒ¡ãƒ¼ã‚«ãƒ¼ã‚’å–å¾—
        maker = ""
        maker_elem = soup.select_one('.maker, .brand, [data-maker]')
        if maker_elem:
            maker = maker_elem.get_text(strip=True)
        
        # ã‚·ãƒªãƒ¼ã‚ºã‚’å–å¾—
        series = ""
        series_elem = soup.select_one('.series, [data-series]')
        if series_elem:
            series = series_elem.get_text(strip=True)
        
        # ã‚¿ã‚°ã‚’å–å¾—
        tags = []
        tag_elems = soup.select('.tag, [data-tag]')
        for elem in tag_elems:
            tag = elem.get_text(strip=True)
            if tag and tag not in tags:
                tags.append(tag)
        
        # ä½œå“ç‰¹å¾´ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§çµåˆ
        if actresses:
            keywords.extend(actresses)
        if genres:
            keywords.extend(genres)
        if maker:
            keywords.append(f"ãƒ¡ãƒ¼ã‚«ãƒ¼: {maker}")
        if series:
            keywords.append(f"ã‚·ãƒªãƒ¼ã‚º: {series}")
        if tags:
            keywords.extend(tags)
        
        keywords_str = "ã€".join(keywords) if keywords else "ä¸æ˜"
        
        # ãƒ¡ã‚¤ãƒ³ç”»åƒURLï¼ˆãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç”»åƒï¼‰ã‚’å–å¾—
        main_image_url = ""
        # è¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ç”»åƒã‚’æ¢ã™
        image_selectors = [
            '.itemBox img[src*="pics.dmm.co.jp"]',
            '.itemBox .package img',
            '.package img',
            'img[src*="pics.dmm.co.jp"]',
            'meta[property="og:image"]'
        ]
        
        for selector in image_selectors:
            if selector.startswith('meta'):
                img_elem = soup.select_one(selector)
                if img_elem:
                    main_image_url = img_elem.get('content', '')
            else:
                img_elem = soup.select_one(selector)
                if img_elem:
                    main_image_url = img_elem.get('src', '') or img_elem.get('data-src', '')
            
            if main_image_url:
                # ç›¸å¯¾URLã®å ´åˆã¯çµ¶å¯¾URLã«å¤‰æ›
                if main_image_url.startswith('//'):
                    main_image_url = 'https:' + main_image_url
                elif main_image_url.startswith('/'):
                    main_image_url = 'https://www.dmm.co.jp' + main_image_url
                break
        
        # ã‚µãƒ³ãƒ—ãƒ«ç”»åƒURLãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆæ‹¡å¤§ç”»åƒï¼‰
        sample_images = []
        
        # ã‚µãƒ³ãƒ—ãƒ«ç”»åƒã®ã‚»ãƒ¬ã‚¯ã‚¿ã‚’è¤‡æ•°è©¦ã™
        sample_selectors = [
            '.sampleImage img',
            '.sample img',
            '.gallery img',
            '[data-sample] img',
            'img[src*="sample"]',
            'img[src*="jp-"]'
        ]
        
        for selector in sample_selectors:
            img_elems = soup.select(selector)
            for img_elem in img_elems:
                img_url = img_elem.get('src', '') or img_elem.get('data-src', '') or img_elem.get('data-original', '')
                if img_url and 'pics.dmm.co.jp' in img_url:
                    # ã‚µãƒ ãƒã‚¤ãƒ«ã§ã¯ãªãæ‹¡å¤§ç”»åƒã‚’å–å¾—
                    # ã‚µãƒ ãƒã‚¤ãƒ«URLã‚’æ‹¡å¤§ç”»åƒURLã«å¤‰æ›
                    if 'thumb' in img_url or 'small' in img_url:
                        img_url = img_url.replace('thumb', '').replace('small', '')
                    # ç›¸å¯¾URLã®å ´åˆã¯çµ¶å¯¾URLã«å¤‰æ›
                    if img_url.startswith('//'):
                        img_url = 'https:' + img_url
                    elif img_url.startswith('/'):
                        img_url = 'https://www.dmm.co.jp' + img_url
                    
                    if img_url not in sample_images:
                        sample_images.append(img_url)
        
        # content_idã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ç”»åƒURLã‚’ç”Ÿæˆï¼ˆãƒšãƒ¼ã‚¸ã‹ã‚‰å–å¾—ã§ããªã„å ´åˆï¼‰
        if not sample_images and content_id:
            # DMMã®ã‚µãƒ³ãƒ—ãƒ«ç”»åƒURLãƒ‘ã‚¿ãƒ¼ãƒ³
            base_url = f"https://pics.dmm.co.jp/digital/videoa/{content_id}/{content_id}jp-"
            for i in range(1, 7):  # 1-6æšç›®
                sample_images.append(f"{base_url}{i}.jpg")
        
        return {
            "title": title,
            "content_id": content_id or "unknown",
            "description": description,
            "keywords": keywords_str,
            "main_image_url": main_image_url,
            "sample_images": sample_images,
            "url": url
        }
        
    except requests.RequestException as e:
        print(f"âŒ ãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
        return None
    except Exception as e:
        print(f"âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
        return None


def generate_cursor_prompt(product_info: dict) -> str:
    """
    Cursorç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
    
    Args:
        product_info: ä½œå“æƒ…å ±ã®è¾æ›¸
        
    Returns:
        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ–‡å­—åˆ—
    """
    title = product_info.get("title", "")
    description = product_info.get("description", "")
    content_id = product_info.get("content_id", "")
    url = product_info.get("url", "")
    keywords = product_info.get("keywords", "")
    main_image_url = product_info.get("main_image_url", "")
    sample_images = product_info.get("sample_images", [])
    
    # ã‚µãƒ³ãƒ—ãƒ«ç”»åƒURLãƒªã‚¹ãƒˆã‚’æ•´å½¢
    sample_images_list = ""
    if sample_images:
        for i, img_url in enumerate(sample_images[:10], 1):  # æœ€å¤§10æš
            sample_images_list += f"   {i}. {img_url}\n"
    else:
        sample_images_list = "   ï¼ˆã‚µãƒ³ãƒ—ãƒ«ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼‰\n"
    
    # ä»Šæ—¥ã®æ—¥ä»˜ã‚’å–å¾—
    today = datetime.now().strftime("%Y-%m-%d")
    
    prompt = f"""# ä¾é ¼
ä»¥ä¸‹ã®ä½œå“ãƒ‡ãƒ¼ã‚¿ï¼ˆç‰¹ã«ã€Œç´¹ä»‹æ–‡ã€ã¨ã€Œä½œå“ç‰¹å¾´ã€ï¼‰ã‚’å…ƒã«ã€ä¾‹ã®ã€Œï½—ã€å¤šã‚ã®è¦–è´è€…ãƒ¢ãƒ¼ãƒ‰ã§ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’æ›¸ã„ã¦ã€‚
å†…å®¹ã¯ã€Œãã‚Œã„ãªã¾ã¨ã‚ã€ã«ã›ãšã€ä¸€äººã®ãƒ•ã‚¡ãƒ³ã¨ã—ã¦èˆˆå¥®ã‚’çˆ†ç™ºã•ã›ã¦ï½—

# ä½œå“ãƒ‡ãƒ¼ã‚¿
- ä½œå“åï¼š {title}
- ç´¹ä»‹æ–‡ï¼š {description}
- ä½œå“IDï¼š {content_id}
- ä½œå“URLï¼š {url}
- ä½œå“ç‰¹å¾´ï¼š {keywords}

- ãƒ¡ã‚¤ãƒ³ç”»åƒURLï¼š {main_image_url}
- ã‚µãƒ³ãƒ—ãƒ«ç”»åƒURLãƒªã‚¹ãƒˆï¼š
{sample_images_list}

# æŒ‡ç¤º
1. **åŸ·ç­†ãƒ«ãƒ¼ãƒ«:**
   - ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å›é¿ã®ãŸã‚ã€ã‚¨ãƒ­ãƒ¯ãƒ¼ãƒ‰ã¯ [K1][K2][K3] ç­‰ã§ä¼ã›å­—ã«ã—ã¦ã€‚
   - ã€Œç´¹ä»‹æ–‡ã€ã«ã‚ã‚‹ã‚·ãƒãƒ¥ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚„ã€ã€Œä½œå“ç‰¹å¾´ã€ã«ã‚ã‚‹å¥³å„ªã®å±æ€§ã‚’ã—ã£ã‹ã‚Šæœ¬æ–‡ã«ç››ã‚Šè¾¼ã‚“ã§ã€‚

2. **ç”»åƒã®é…ç½®:**
   - è¨˜äº‹å†’é ­ã«ãƒ¡ã‚¤ãƒ³ç”»åƒã‚’ `![{title}]({main_image_url})` ã§é…ç½®ã€‚
   - è¨˜äº‹ã®é€”ä¸­ã«ã‚µãƒ³ãƒ—ãƒ«ç”»åƒã‚’ãƒªã‚¹ãƒˆã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«æ•°æšé¸ã‚“ã§é…ç½®ï¼ˆURLã¯æ”¹å¤‰ä¸å¯ï¼‰ã€‚

3. **ä¿å­˜å‡¦ç†:**
   - è¨˜äº‹ãŒå®Œæˆã—ãŸã‚‰ã€ä»¥å‰ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆåŒæ§˜ã®ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆç½®æ›ãƒ»å‹•ç”»ã‚³ãƒ¼ãƒ‰è¿½åŠ ï¼‰ã‚’é©ç”¨ã—ãŸä¸Šã§ã€ä»¥ä¸‹ã®ãƒ‘ã‚¹ã«ä¿å­˜ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œï¼ˆã¾ãŸã¯ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆï¼‰ã—ã¦ã€‚
   - ä¿å­˜ãƒ‘ã‚¹: `/Users/takahashitoshifumi/Desktop/Adult-affi/content/{today}-{content_id}.md`
   - â€» `{today}` ã¯å®Ÿè¡Œæ—¥ã®æ—¥ä»˜ (YYYY-MM-DD)
"""
    
    return prompt


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("\n" + "=" * 80)
    print("  DMMä½œå“URL â†’ Cursorç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆãƒ„ãƒ¼ãƒ«")
    print("=" * 80 + "\n")
    
    # URLå…¥åŠ›
    url = input("ä½œå“URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
    
    if not url:
        print("âŒ URLãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“", file=sys.stderr)
        sys.exit(1)
    
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
    print("\nğŸ” ä½œå“æƒ…å ±ã‚’å–å¾—ä¸­...")
    product_info = scrape_dmm_product_info(url)
    
    if not product_info:
        print("âŒ ä½œå“æƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ", file=sys.stderr)
        sys.exit(1)
    
    # å–å¾—ã—ãŸæƒ…å ±ã‚’è¡¨ç¤º
    print("\nâœ… å–å¾—ã—ãŸæƒ…å ±:")
    print(f"   ä½œå“å: {product_info.get('title', 'ä¸æ˜')}")
    print(f"   ä½œå“ID: {product_info.get('content_id', 'ä¸æ˜')}")
    print(f"   ç´¹ä»‹æ–‡: {product_info.get('description', 'ä¸æ˜')[:100]}...")
    print(f"   ä½œå“ç‰¹å¾´: {product_info.get('keywords', 'ä¸æ˜')[:100]}...")
    print(f"   ãƒ¡ã‚¤ãƒ³ç”»åƒ: {product_info.get('main_image_url', 'ä¸æ˜')[:80]}...")
    print(f"   ã‚µãƒ³ãƒ—ãƒ«ç”»åƒ: {len(product_info.get('sample_images', []))}æš")
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
    print("\nğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆä¸­...")
    prompt = generate_cursor_prompt(product_info)
    
    # ã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰ã«ã‚³ãƒ”ãƒ¼
    try:
        pyperclip.copy(prompt)
        print("\nâœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰ã«ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸï¼")
        print("\n" + "=" * 80)
        print("ç”Ÿæˆã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:")
        print("=" * 80)
        print(prompt)
        print("=" * 80)
        print("\nğŸ’¡ Cursorã®ãƒãƒ£ãƒƒãƒˆæ¬„ã«è²¼ã‚Šä»˜ã‘ã¦ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚")
    except Exception as e:
        print(f"\nâš ï¸  ã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰ã¸ã®ã‚³ãƒ”ãƒ¼ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", file=sys.stderr)
        print("\nç”Ÿæˆã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:")
        print("=" * 80)
        print(prompt)
        print("=" * 80)
        print("\nğŸ’¡ ä¸Šè¨˜ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ‰‹å‹•ã§ã‚³ãƒ”ãƒ¼ã—ã¦ãã ã•ã„ã€‚")


if __name__ == "__main__":
    main()

