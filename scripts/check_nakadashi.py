#!/usr/bin/env python3
"""
æ—¢å­˜è¨˜äº‹ã®URLã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã€ã€Œä¸­å‡ºã—ã€ä½œå“ã‹ã©ã†ã‹ã‚’èª¿ã¹ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import re
import sys
import time
from pathlib import Path
from urllib.parse import urlparse, parse_qs
import urllib.request
import urllib.error
from bs4 import BeautifulSoup

def extract_content_id_from_url(url: str) -> str:
    """ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆURLã‹ã‚‰content_idã‚’æŠ½å‡º"""
    try:
        # URLã‚’ãƒ‘ãƒ¼ã‚¹
        parsed = urlparse(url)
        
        # lurlãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰content_idã‚’æŠ½å‡º
        if 'lurl' in parse_qs(parsed.query):
            lurl = parse_qs(parsed.query)['lurl'][0]
            # URLãƒ‡ã‚³ãƒ¼ãƒ‰
            from urllib.parse import unquote
            decoded_lurl = unquote(lurl)
            # content_idã‚’æŠ½å‡ºï¼ˆid=ã®å¾Œï¼‰
            match = re.search(r'id=([^&/]+)', decoded_lurl)
            if match:
                return match.group(1)
        
        # ç›´æ¥URLã‹ã‚‰æŠ½å‡º
        match = re.search(r'id=([^&/]+)', url)
        if match:
            return match.group(1)
            
    except Exception as e:
        print(f"âš ï¸  URLè§£æã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
    
    return None


def get_dmm_page_content(content_id: str) -> str:
    """DMMã®ä½œå“ãƒšãƒ¼ã‚¸ã®HTMLã‚’å–å¾—"""
    # DMMã®ä½œå“ãƒšãƒ¼ã‚¸URLï¼ˆvideoaã¨videoã®ä¸¡æ–¹ã‚’è©¦ã™ï¼‰
    urls = [
        f"https://www.dmm.co.jp/digital/videoa/-/detail/=/cid={content_id}/",
        f"https://www.dmm.co.jp/digital/video/-/detail/=/cid={content_id}/",
    ]
    
    # SSLè¨¼æ˜æ›¸æ¤œè¨¼ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆé–‹ç™ºç’°å¢ƒã®ã¿ï¼‰
    import ssl
    ssl_context = ssl.create_default_context()
    ssl_context.check_hostname = False
    ssl_context.verify_mode = ssl.CERT_NONE
    
    # User-Agentã‚’è¨­å®šï¼ˆDMMãŒãƒ–ãƒ­ãƒƒã‚¯ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ï¼‰
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    # è¤‡æ•°ã®URLã‚’è©¦ã™
    for url in urls:
        try:
            req = urllib.request.Request(url, headers=headers)
            with urllib.request.urlopen(req, timeout=10, context=ssl_context) as response:
                html = response.read().decode('utf-8')
                # HTMLãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆç©ºã§ãªã„ã€ã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ã§ãªã„ï¼‰
                if html and len(html) > 1000 and '404' not in html.lower()[:500]:
                    return html
        except urllib.error.HTTPError as e:
            # 404ã®å ´åˆã¯æ¬¡ã®URLã‚’è©¦ã™
            if e.code == 404:
                continue
            print(f"âš ï¸  HTTPã‚¨ãƒ©ãƒ¼ ({content_id}, {url}): {e.code} - {e.reason}", file=sys.stderr)
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯æ¬¡ã®URLã‚’è©¦ã™
            continue
    
    # ã™ã¹ã¦ã®URLã§å¤±æ•—
    print(f"âš ï¸  å–å¾—ã‚¨ãƒ©ãƒ¼ ({content_id}): ã™ã¹ã¦ã®URLã§å–å¾—ã«å¤±æ•—", file=sys.stderr)
    return None


def check_nakadashi(html: str) -> bool:
    """HTMLã‹ã‚‰ã€Œä¸­å‡ºã—ã€ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆè©³ç´°ç‰ˆï¼‰"""
    if not html:
        return False
    
    # BeautifulSoupã§ãƒ‘ãƒ¼ã‚¹
    soup = BeautifulSoup(html, 'html.parser')
    
    # ã€Œä¸­å‡ºã—ã€ã¨ã„ã†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œç´¢
    nakadashi_keywords = ['ä¸­å‡ºã—', 'ä¸­å‡º', 'ä¸­ã ã—', 'ä¸­ã å‡ºã—']
    
    # 1. ã‚¸ãƒ£ãƒ³ãƒ«ã‚¿ã‚°ã‚’è©³ç´°ã«ç¢ºèª
    # DMMã®ã‚¸ãƒ£ãƒ³ãƒ«ã‚¿ã‚°ã¯é€šå¸¸ã€ç‰¹å®šã®ã‚¯ãƒ©ã‚¹ã‚„IDã‚’æŒã¤
    genre_selectors = [
        'table.mg-bg',
        'table[summary="ã‚¸ãƒ£ãƒ³ãƒ«"]',
        'div[class*="genre"]',
        'div[class*="tag"]',
        'span[class*="genre"]',
        'a[class*="genre"]',
        'td[class*="genre"]',
        'ul[class*="genre"]',
        'li[class*="genre"]',
    ]
    
    for selector in genre_selectors:
        elements = soup.select(selector)
        for elem in elements:
            elem_text = elem.get_text()
            for keyword in nakadashi_keywords:
                if keyword in elem_text:
                    return True
    
    # 2. ä½œå“èª¬æ˜æ–‡ã‚’ç¢ºèª
    description_selectors = [
        'div[class*="description"]',
        'div[class*="comment"]',
        'div[class*="review"]',
        'p[class*="description"]',
        'td[class*="description"]',
        'div#mu',
        'div[class*="mu"]',
    ]
    
    for selector in description_selectors:
        elements = soup.select(selector)
        for elem in elements:
            elem_text = elem.get_text()
            for keyword in nakadashi_keywords:
                if keyword in elem_text:
                    return True
    
    # 3. ã‚¿ã‚¤ãƒˆãƒ«ã‚„è¦‹å‡ºã—ã‚’ç¢ºèª
    title_selectors = ['h1', 'h2', 'h3', 'title']
    for selector in title_selectors:
        elements = soup.select(selector)
        for elem in elements:
            elem_text = elem.get_text()
            for keyword in nakadashi_keywords:
                if keyword in elem_text:
                    return True
    
    # 4. ã™ã¹ã¦ã®ãƒªãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆã‚’ç¢ºèª
    links = soup.find_all('a')
    for link in links:
        link_text = link.get_text()
        for keyword in nakadashi_keywords:
            if keyword in link_text:
                return True
    
    # 5. ãƒšãƒ¼ã‚¸å…¨ä½“ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ç¢ºèªï¼ˆæœ€å¾Œã®æ‰‹æ®µï¼‰
    text = soup.get_text()
    for keyword in nakadashi_keywords:
        if keyword in text:
            return True
    
    return False


def parse_markdown_file(file_path: Path) -> dict:
    """Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰frontmatterã‚’è§£æ"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # frontmatterã‚’æŠ½å‡º
        match = re.match(r'^---\n(.*?)\n---', content, re.DOTALL)
        if not match:
            return {}
        
        frontmatter_text = match.group(1)
        frontmatter = {}
        
        # å„è¡Œã‚’ãƒ‘ãƒ¼ã‚¹
        for line in frontmatter_text.split('\n'):
            if ':' in line:
                key, value = line.split(':', 1)
                key = key.strip()
                value = value.strip().strip('"').strip("'")
                frontmatter[key] = value
        
        return frontmatter
        
    except Exception as e:
        print(f"âš ï¸  ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({file_path}): {e}", file=sys.stderr)
        return {}


def add_nakadashi_tag_to_article(file_path: Path) -> bool:
    """è¨˜äº‹ãƒ•ã‚¡ã‚¤ãƒ«ã«ã€Œä¸­å‡ºã—ã€ã‚¿ã‚°ã‚’è¿½åŠ ã—ã€è¨˜äº‹æœ¬æ–‡ã«ã‚‚æƒ…å ±ã‚’è¿½åŠ """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # frontmatterã‚’æŠ½å‡º
        match = re.match(r'^(---\n.*?\n---)', content, re.DOTALL)
        if not match:
            return False
        
        frontmatter_text = match.group(1)
        rest_content = content[len(frontmatter_text):]
        
        # æ—¢ã«ã€Œä¸­å‡ºã—ã€ã‚¿ã‚°ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        tag_already_exists = '"ä¸­å‡ºã—"' in frontmatter_text or "'ä¸­å‡ºã—'" in frontmatter_text
        
        # tagsè¡Œã‚’æ¢ã™
        tags_pattern = r'tags:\s*\[(.*?)\]'
        tags_match = re.search(tags_pattern, frontmatter_text)
        
        new_frontmatter = frontmatter_text
        if not tag_already_exists:
            if tags_match:
                # æ—¢å­˜ã®tagsã«ã€Œä¸­å‡ºã—ã€ã‚’è¿½åŠ 
                existing_tags = tags_match.group(1)
                # æ—¢å­˜ã®ã‚¿ã‚°ã®å¾Œã«ã€Œä¸­å‡ºã—ã€ã‚’è¿½åŠ 
                new_tags = existing_tags.rstrip() + ', "ä¸­å‡ºã—"'
                new_frontmatter = re.sub(tags_pattern, f'tags: [{new_tags}]', frontmatter_text)
            else:
                # tagsè¡ŒãŒãªã„å ´åˆã¯è¿½åŠ 
                new_frontmatter = frontmatter_text.rstrip() + '\ntags: ["ä¸­å‡ºã—"]\n---'
        
        # è¨˜äº‹æœ¬æ–‡ã«ã€Œä¸­å‡ºã—ã€ã®æƒ…å ±ã‚’è¿½åŠ ï¼ˆæ—¢ã«å«ã¾ã‚Œã¦ã„ãªã„å ´åˆï¼‰
        new_rest_content = rest_content
        if 'ä¸­å‡ºã—' not in rest_content and '[K1]' not in rest_content:
            # ã€Œã“ã“ãŒã‚¨ãƒ­ã‹ã£ãŸï½—ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®å¾Œã«è¿½åŠ 
            ero_section_pattern = r'(## ã“ã“ãŒã‚¨ãƒ­ã‹ã£ãŸï½—.*?\n)'
            ero_match = re.search(ero_section_pattern, rest_content, re.DOTALL)
            
            if ero_match:
                # ã€Œã“ã“ãŒã‚¨ãƒ­ã‹ã£ãŸï½—ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®å¾Œã«è¿½åŠ 
                insert_pos = ero_match.end()
                nakadashi_note = '\n**ğŸ¯ ä¸­å‡ºã—ä½œå“**\n\nã“ã®ä½œå“ã¯[K1]ã‚·ãƒ¼ãƒ³ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚\n\n'
                new_rest_content = rest_content[:insert_pos] + nakadashi_note + rest_content[insert_pos:]
            else:
                # ã€Œã“ã“ãŒã‚¨ãƒ­ã‹ã£ãŸï½—ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒãªã„å ´åˆã¯ã€æœ€åˆã®è¦‹å‡ºã—ã®å¾Œã«è¿½åŠ 
                first_heading_pattern = r'(## .*?\n)'
                first_heading_match = re.search(first_heading_pattern, rest_content)
                if first_heading_match:
                    insert_pos = first_heading_match.end()
                    nakadashi_note = '\n**ğŸ¯ ä¸­å‡ºã—ä½œå“**\n\nã“ã®ä½œå“ã¯[K1]ã‚·ãƒ¼ãƒ³ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚\n\n'
                    new_rest_content = rest_content[:insert_pos] + nakadashi_note + rest_content[insert_pos:]
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›¸ãè¾¼ã¿
        new_content = new_frontmatter + new_rest_content
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(new_content)
        
        return True
        
    except Exception as e:
        print(f"âš ï¸  ã‚¿ã‚°è¿½åŠ ã‚¨ãƒ©ãƒ¼ ({file_path}): {e}", file=sys.stderr)
        return False


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    script_dir = Path(__file__).parent
    project_root = script_dir.parent
    content_dir = project_root / "content"
    
    if not content_dir.exists():
        print(f"âŒ contentãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {content_dir}")
        sys.exit(1)
    
    # ã™ã¹ã¦ã®Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    md_files = sorted(content_dir.glob("*.md"))
    
    print(f"ğŸ“ è¨˜äº‹ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(md_files)}ä»¶")
    print("=" * 80)
    
    nakadashi_count = 0
    checked_count = 0
    error_count = 0
    updated_count = 0
    
    results = []
    
    for idx, md_file in enumerate(md_files, 1):
        print(f"\n[{idx}/{len(md_files)}] {md_file.name} ã‚’ãƒã‚§ãƒƒã‚¯ä¸­...")
        
        # frontmatterã‚’è§£æ
        frontmatter = parse_markdown_file(md_file)
        
        # content_idã‚’å–å¾—
        content_id = frontmatter.get('contentId', '')
        affiliate_link = frontmatter.get('affiliateLink', '')
        
        if not content_id and affiliate_link:
            content_id = extract_content_id_from_url(affiliate_link)
        
        if not content_id:
            print(f"   âš ï¸  content_idãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            error_count += 1
            continue
        
        # DMMãƒšãƒ¼ã‚¸ã‚’å–å¾—
        html = get_dmm_page_content(content_id)
        
        if not html:
            print(f"   âš ï¸  ãƒšãƒ¼ã‚¸å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            error_count += 1
            continue
        
        # ã€Œä¸­å‡ºã—ã€ã‚’ãƒã‚§ãƒƒã‚¯
        is_nakadashi = check_nakadashi(html)
        
        checked_count += 1
        
        if is_nakadashi:
            nakadashi_count += 1
            print(f"   âœ… ä¸­å‡ºã—ä½œå“ã§ã™")
            
            # ã‚¿ã‚°ã‚’è¿½åŠ 
            if add_nakadashi_tag_to_article(md_file):
                print(f"   ğŸ“ ã€Œä¸­å‡ºã—ã€ã‚¿ã‚°ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
                updated_count += 1
            else:
                print(f"   â„¹ï¸  æ—¢ã«ã€Œä¸­å‡ºã—ã€ã‚¿ã‚°ãŒè¿½åŠ æ¸ˆã¿ã§ã™")
            
            results.append({
                'file': md_file.name,
                'content_id': content_id,
                'title': frontmatter.get('title', ''),
                'is_nakadashi': True
            })
        else:
            print(f"   âŒ ä¸­å‡ºã—ä½œå“ã§ã¯ã‚ã‚Šã¾ã›ã‚“")
        
        # APIè² è·è»½æ¸›ã®ãŸã‚ã€å°‘ã—å¾…æ©Ÿ
        time.sleep(1)
    
    # çµæœã‚’è¡¨ç¤º
    print("\n" + "=" * 80)
    print("ğŸ“Š ãƒã‚§ãƒƒã‚¯çµæœ")
    print("=" * 80)
    print(f"âœ… ãƒã‚§ãƒƒã‚¯å®Œäº†: {checked_count}ä»¶")
    print(f"ğŸ¯ ä¸­å‡ºã—ä½œå“: {nakadashi_count}ä»¶")
    print(f"ğŸ“ ã‚¿ã‚°è¿½åŠ : {updated_count}ä»¶")
    print(f"âŒ ã‚¨ãƒ©ãƒ¼: {error_count}ä»¶")
    
    if results:
        print("\nğŸ“ ä¸­å‡ºã—ä½œå“ä¸€è¦§:")
        for result in results:
            print(f"   - {result['content_id']}: {result['title'][:50]}...")
    
    # çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    import json
    output_file = project_root / "data" / "nakadashi_check_results.json"
    output_file.parent.mkdir(exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            'total_checked': checked_count,
            'nakadashi_count': nakadashi_count,
            'error_count': error_count,
            'results': results
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_file}")


if __name__ == "__main__":
    main()

